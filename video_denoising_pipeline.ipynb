{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Denoising Pipeline\n",
    "### Classical DSP vs. Deep Learning (DnCNN) — Frame-by-Frame Video Processing\n",
    "\n",
    "This notebook builds a complete video denoising pipeline targeting the following skills:\n",
    "- **Data pipeline** for training, validation, and testing\n",
    "- **Classical DSP baselines**: Gaussian, Wiener, Total Variation\n",
    "- **Deep learning denoiser**: DnCNN architecture in PyTorch\n",
    "- **Quantitative evaluation**: PSNR & SSIM comparison\n",
    "- **Video processing**: frame-by-frame inference and video reconstruction\n",
    "\n",
    "---\n",
    "**Dependencies:**\n",
    "```\n",
    "pip install torch torchvision numpy scipy matplotlib scikit-image opencv-python Pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Download assets from GitHub (Colab setup) ─────────────────────────────────\n",
    "import os\n",
    "if not os.path.exists('clip.mp4'):\n",
    "    !wget -q https://raw.githubusercontent.com/preeti-chauhan/video-denoising-pipeline/main/clip.mp4\n",
    "    print('Downloaded clip.mp4')\n",
    "if not os.path.exists('dncnn_weights.pth'):\n",
    "    !wget -q https://raw.githubusercontent.com/preeti-chauhan/video-denoising-pipeline/main/dncnn_weights.pth\n",
    "    print('Downloaded dncnn_weights.pth')\n",
    "print('Assets ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from scipy import ndimage, signal\n",
    "from skimage import data as skdata, color, util, restoration, filters\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'PyTorch: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 1 · Data Pipeline\n\nWe use a real video clip (`clip.mp4`) as clean reference frames. The pipeline:\n1. Extract all frames from `clip.mp4` → grayscale float32\n2. Add synthetic Gaussian noise on-the-fly during training → noisy patches\n3. Wrap in a `torch.utils.data.Dataset` for batched training\n\n> **Colab:** Upload your own video to replace `clip.mp4`, or swap in any public-domain clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Parameters ────────────────────────────────────────────────────────────────\nNOISE_STD   = 25 / 255.0   # ← tune: noise level (25/255 is visually noticeable)\nPATCH_SIZE  = 64            # ← tune: training patch size\nBATCH_SIZE  = 32\nNUM_EPOCHS  = 20            # ← tune: increase for better results\nLR          = 1e-3\n\n# ── Load clean frames from real video ─────────────────────────────────────────\ndef load_frames_from_video(path, max_frames=None):\n    \"\"\"Extract every frame from a video as grayscale float32.\"\"\"\n    cap = cv2.VideoCapture(path)\n    frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n        frames.append(gray)\n        if max_frames and len(frames) >= max_frames:\n            break\n    cap.release()\n    return frames\n\nVIDEO_PATH   = 'clip.mp4'          # 5-second Big Buck Bunny clip (public domain)\nclean_frames = load_frames_from_video(VIDEO_PATH)\nprint(f'Loaded {len(clean_frames)} clean frames — shape: {clean_frames[0].shape}')\n\n# Preview 12 evenly-spaced frames\nidxs = [int(i * len(clean_frames) / 12) for i in range(12)]\nfig, axes = plt.subplots(2, 6, figsize=(18, 6))\nfor ax, idx in zip(axes.flat, idxs):\n    ax.imshow(clean_frames[idx], cmap='gray')\n    ax.axis('off')\nfig.suptitle('Clean Frames from clip.mp4', fontweight='bold')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDenoiseDataset(Dataset):\n    \"\"\"Extracts random patches from clean frames and adds Gaussian noise.\"\"\"\n\n    def __init__(self, frames, patch_size=64, patches_per_frame=50, noise_std=25/255.0):\n        self.patches     = []\n        self.patch_size  = patch_size\n        self.noise_std   = noise_std\n\n        for frame in frames:\n            H, W = frame.shape\n            for _ in range(patches_per_frame):\n                r = np.random.randint(0, H - patch_size)\n                c = np.random.randint(0, W - patch_size)\n                patch = frame[r:r+patch_size, c:c+patch_size]\n                self.patches.append(patch)\n\n    def __len__(self):\n        return len(self.patches)\n\n    def __getitem__(self, idx):\n        clean = self.patches[idx]\n        noise = np.random.normal(0, self.noise_std, clean.shape).astype(np.float32)\n        noisy = np.clip(clean + noise, 0, 1)\n        # Add channel dim: (1, H, W)\n        clean_t = torch.from_numpy(clean).unsqueeze(0)\n        noisy_t = torch.from_numpy(noisy).unsqueeze(0)\n        noise_t = torch.from_numpy(noise).unsqueeze(0)  # DnCNN predicts noise\n        return noisy_t, clean_t, noise_t\n\n# ── Train / val / test split (149 frames: 80% / 10% / 10%) ──────────────────\nn = len(clean_frames)\nn_train = int(n * 0.8)\nn_val   = int(n * 0.1)\n\ntrain_frames = clean_frames[:n_train]\nval_frames   = clean_frames[n_train:n_train + n_val]\ntest_frames  = clean_frames[n_train + n_val:]\n\ntrain_ds = VideoDenoiseDataset(train_frames, PATCH_SIZE, patches_per_frame=50, noise_std=NOISE_STD)\nval_ds   = VideoDenoiseDataset(val_frames,   PATCH_SIZE, patches_per_frame=50, noise_std=NOISE_STD)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\nprint(f'Train frames: {len(train_frames)} | Val frames: {len(val_frames)} | Test frames: {len(test_frames)}')\nprint(f'Train patches: {len(train_ds)} | Val patches: {len(val_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 · Classical DSP Baselines\n",
    "\n",
    "Before training the CNN, we establish classical denoising baselines to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img, std=NOISE_STD):\n",
    "    noise = np.random.normal(0, std, img.shape).astype(np.float32)\n",
    "    return np.clip(img + noise, 0, 1)\n",
    "\n",
    "def classical_denoise(noisy):\n",
    "    return {\n",
    "        'Gaussian':       filters.gaussian(noisy, sigma=1.5),\n",
    "        'Wiener':         signal.wiener(noisy, mysize=5),\n",
    "        'Total Variation': restoration.denoise_tv_chambolle(noisy, weight=0.1),\n",
    "    }\n",
    "\n",
    "def compute_metrics(clean, candidate):\n",
    "    p = psnr(clean, candidate, data_range=1.0)\n",
    "    s = ssim(clean, candidate, data_range=1.0)\n",
    "    return p, s\n",
    "\n",
    "# Evaluate on test frames\n",
    "test_img   = test_frames[0]\n",
    "noisy_img  = add_noise(test_img)\n",
    "results    = classical_denoise(noisy_img)\n",
    "\n",
    "print(f'  Noisy input       PSNR={compute_metrics(test_img, noisy_img)[0]:.2f} dB  SSIM={compute_metrics(test_img, noisy_img)[1]:.4f}')\n",
    "for name, denoised in results.items():\n",
    "    p, s = compute_metrics(test_img, denoised)\n",
    "    print(f'  {name:<20} PSNR={p:.2f} dB  SSIM={s:.4f}')\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for ax, (img, title) in zip(axes, [\n",
    "    (test_img,  'Clean'),\n",
    "    (noisy_img, f'Noisy (σ={NOISE_STD:.3f})'),\n",
    "    *[(v, k) for k, v in results.items()]\n",
    "]):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Classical DSP Baselines', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 · DnCNN — Deep Learning Denoiser\n",
    "\n",
    "**DnCNN** (Zhang et al., 2017) uses residual learning: the network predicts\n",
    "the **noise** rather than the clean image.\n",
    "\n",
    "```\n",
    "clean = noisy − DnCNN(noisy)\n",
    "```\n",
    "\n",
    "Architecture:\n",
    "- **Layer 1**: Conv(64) + ReLU\n",
    "- **Layers 2–(D-1)**: Conv(64) + BatchNorm + ReLU\n",
    "- **Layer D**: Conv(1) — outputs predicted noise map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnCNN(nn.Module):\n",
    "    \"\"\"DnCNN: Beyond Gaussian Denoiser (Zhang et al., 2017).\"\"\"\n",
    "\n",
    "    def __init__(self, depth=17, n_filters=64, kernel_size=3):\n",
    "        super().__init__()\n",
    "        pad = kernel_size // 2\n",
    "        layers = []\n",
    "\n",
    "        # First layer: Conv + ReLU (no BN)\n",
    "        layers += [nn.Conv2d(1, n_filters, kernel_size, padding=pad, bias=False),\n",
    "                   nn.ReLU(inplace=True)]\n",
    "\n",
    "        # Middle layers: Conv + BN + ReLU\n",
    "        for _ in range(depth - 2):\n",
    "            layers += [nn.Conv2d(n_filters, n_filters, kernel_size, padding=pad, bias=False),\n",
    "                       nn.BatchNorm2d(n_filters),\n",
    "                       nn.ReLU(inplace=True)]\n",
    "\n",
    "        # Last layer: Conv only — outputs noise map\n",
    "        layers += [nn.Conv2d(n_filters, 1, kernel_size, padding=pad, bias=False)]\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise_pred = self.net(x)\n",
    "        return x - noise_pred          # residual: predicted clean image\n",
    "\n",
    "model = DnCNN(depth=17).to(DEVICE)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'DnCNN parameters: {total_params:,}')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 · Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ── Train ──────────────────────────────────────────────────────────────────\n",
    "    model.train()\n",
    "    t_loss = 0.0\n",
    "    for noisy, clean, _ in train_loader:\n",
    "        noisy, clean = noisy.to(DEVICE), clean.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(noisy)\n",
    "        loss   = criterion(output, clean)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "\n",
    "    # ── Validate ───────────────────────────────────────────────────────────────\n",
    "    model.eval()\n",
    "    v_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for noisy, clean, _ in val_loader:\n",
    "            noisy, clean = noisy.to(DEVICE), clean.to(DEVICE)\n",
    "            output = model(noisy)\n",
    "            v_loss += criterion(output, clean).item()\n",
    "\n",
    "    t_loss /= len(train_loader)\n",
    "    v_loss /= len(val_loader)\n",
    "    train_losses.append(t_loss)\n",
    "    val_losses.append(v_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1:>3}/{NUM_EPOCHS}]  Train Loss: {t_loss:.6f}  Val Loss: {v_loss:.6f}  LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "\n",
    "# ── Plot loss curves ──────────────────────────────────────────────────────────\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses,   label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('DnCNN Training Curves')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 · Evaluation — Classical vs. DnCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dncnn_denoise(model, noisy_img):\n",
    "    \"\"\"Run DnCNN inference on a single grayscale image.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t = torch.from_numpy(noisy_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        out = model(t)\n",
    "        out = out.squeeze().cpu().numpy()\n",
    "    return np.clip(out, 0, 1)\n",
    "\n",
    "print('\\n=== Denoising Benchmark (test frames) ===')\n",
    "print(f'{\"Method\":<22} {\"PSNR (dB)\":>10} {\"SSIM\":>8}')\n",
    "print('-' * 42)\n",
    "\n",
    "all_results = {}\n",
    "for test_img in test_frames:\n",
    "    noisy = add_noise(test_img)\n",
    "    classical = classical_denoise(noisy)\n",
    "    cnn_out   = dncnn_denoise(model, noisy.astype(np.float32))\n",
    "\n",
    "    methods = {'Noisy': noisy, **classical, 'DnCNN': cnn_out}\n",
    "    for name, img in methods.items():\n",
    "        p, s = compute_metrics(test_img, img)\n",
    "        all_results.setdefault(name, []).append((p, s))\n",
    "\n",
    "for name, scores in all_results.items():\n",
    "    avg_p = np.mean([s[0] for s in scores])\n",
    "    avg_s = np.mean([s[1] for s in scores])\n",
    "    print(f'{name:<22} {avg_p:>10.2f} {avg_s:>8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visual comparison on one test frame ───────────────────────────────────────\n",
    "test_img  = test_frames[0]\n",
    "noisy_img = add_noise(test_img)\n",
    "classical = classical_denoise(noisy_img)\n",
    "cnn_out   = dncnn_denoise(model, noisy_img.astype(np.float32))\n",
    "\n",
    "imgs   = [test_img, noisy_img, *classical.values(), cnn_out]\n",
    "titles = ['Clean', f'Noisy (σ={NOISE_STD:.3f})', *classical.keys(), 'DnCNN']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(imgs), figsize=(4 * len(imgs), 4))\n",
    "for ax, img, title in zip(axes, imgs, titles):\n",
    "    p, s = compute_metrics(test_img, img)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'{title}\\nPSNR={p:.1f} dB', fontsize=9)\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Classical DSP vs. DnCNN', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 · Video Processing Pipeline\n",
    "\n",
    "Apply the trained DnCNN frame-by-frame to a video.\n",
    "- **Option A**: Use the synthetic video generated below\n",
    "- **Option B**: Swap in your own `.mp4` path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generate synthetic noisy video from clip.mp4 ──────────────────────────────\nSYNTHETIC_VIDEO  = 'synthetic_noisy.mp4'\nDENOISED_VIDEO   = 'denoised_output.mp4'\ncap = cv2.VideoCapture(VIDEO_PATH)\nfps = cap.get(cv2.CAP_PROP_FPS)\nW   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nH   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\ncap.release()\n\n# Write noisy MP4 (H.264 — plays in browser on GitHub)\ntmp_noisy = '/tmp/noisy_raw.mp4'\nout_noisy = cv2.VideoWriter(tmp_noisy,\n                             cv2.VideoWriter_fourcc(*'mp4v'),\n                             fps, (W, H), isColor=False)\n\nnoisy_frames_preview = []\ncap = cv2.VideoCapture(VIDEO_PATH)\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n    noisy = np.clip(gray + np.random.normal(0, NOISE_STD, gray.shape).astype(np.float32), 0, 1)\n    out_noisy.write((noisy * 255).astype(np.uint8))\n    if len(noisy_frames_preview) < 5:\n        noisy_frames_preview.append(noisy)\ncap.release()\nout_noisy.release()\n\n# Re-encode with ffmpeg for proper H.264/AAC so GitHub can play it\nos.system(f'ffmpeg -y -i {tmp_noisy} -vcodec libx264 -crf 23 -pix_fmt yuv420p {SYNTHETIC_VIDEO} -loglevel quiet')\nprint(f'Noisy video saved: {SYNTHETIC_VIDEO}')\n\n# Preview\nfig, axes = plt.subplots(1, 5, figsize=(18, 4))\nfor ax, frame in zip(axes, noisy_frames_preview):\n    ax.imshow(frame, cmap='gray'); ax.axis('off')\nfig.suptitle('Sample Noisy Frames (σ=25/255)', fontweight='bold')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_path, output_path, model):\n    \"\"\"Read noisy video frame-by-frame, denoise with DnCNN, write MP4 output.\"\"\"\n    cap   = cv2.VideoCapture(input_path)\n    fps   = cap.get(cv2.CAP_PROP_FPS)\n    W     = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    H     = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    tmp_out = '/tmp/denoised_raw.mp4'\n    out = cv2.VideoWriter(tmp_out, cv2.VideoWriter_fourcc(*'mp4v'), fps, (W, H), isColor=False)\n\n    denoised_preview = []\n    noisy_preview    = []\n    model.eval()\n\n    for i in range(total):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n        denoised = dncnn_denoise(model, gray)\n        out.write((denoised * 255).astype(np.uint8))\n        if i < 5:\n            noisy_preview.append(gray)\n            denoised_preview.append(denoised)\n\n    cap.release()\n    out.release()\n\n    # Re-encode for proper H.264 playback\n    os.system(f'ffmpeg -y -i {tmp_out} -vcodec libx264 -crf 23 -pix_fmt yuv420p {output_path} -loglevel quiet')\n    print(f'Denoised video saved: {output_path}  ({total} frames)')\n    return noisy_preview, denoised_preview\n\nnoisy_preview, denoised_preview = process_video(SYNTHETIC_VIDEO, DENOISED_VIDEO, model)\n\n# ── Visualize sample frames ───────────────────────────────────────────────────\nfig, axes = plt.subplots(2, 5, figsize=(18, 6))\nfor i in range(5):\n    axes[0, i].imshow(noisy_preview[i],    cmap='gray')\n    axes[0, i].set_title(f'Noisy frame {i+1}')\n    axes[0, i].axis('off')\n    axes[1, i].imshow(denoised_preview[i], cmap='gray')\n    axes[1, i].set_title(f'Denoised frame {i+1}')\n    axes[1, i].axis('off')\nfig.suptitle('Video Frame Denoising — DnCNN', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 · Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save ──────────────────────────────────────────────────────────────────────\n",
    "torch.save(model.state_dict(), 'dncnn_weights.pth')\n",
    "print('Model saved to dncnn_weights.pth')\n",
    "\n",
    "# ── Load (example) ────────────────────────────────────────────────────────────\n",
    "# model_loaded = DnCNN(depth=17).to(DEVICE)\n",
    "# model_loaded.load_state_dict(torch.load('dncnn_weights.pth', map_location=DEVICE))\n",
    "# model_loaded.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}