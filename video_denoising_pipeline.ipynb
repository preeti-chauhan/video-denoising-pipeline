{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Denoising Pipeline\n",
    "### Classical DSP vs. Deep Learning (DnCNN) — Frame-by-Frame Video Processing\n",
    "\n",
    "This notebook builds a complete video denoising pipeline targeting the following skills:\n",
    "- **Data pipeline** for training, validation, and testing\n",
    "- **Classical DSP baselines**: Gaussian, Wiener, Total Variation\n",
    "- **Deep learning denoiser**: DnCNN architecture in PyTorch\n",
    "- **Quantitative evaluation**: PSNR & SSIM comparison\n",
    "- **Video processing**: frame-by-frame inference and video reconstruction\n",
    "\n",
    "---\n",
    "**Dependencies:**\n",
    "```\n",
    "pip install torch torchvision numpy scipy matplotlib scikit-image opencv-python Pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "PyTorch: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from scipy import ndimage, signal\n",
    "from skimage import data as skdata, color, util, restoration, filters\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'PyTorch: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Parameters ────────────────────────────────────────────────────────────────\nNOISE_STD   = 25 / 255.0   # ← tune: noise level (25/255 is visually noticeable)\nPATCH_SIZE  = 64            # ← tune: training patch size\nBATCH_SIZE  = 32\nNUM_EPOCHS  = 20            # ← tune: increase for better results\nLR          = 1e-3\n\n# ── Load clean images ─────────────────────────────────────────────────────────\ndef load_clean_frames():\n    \"\"\"Load skimage sample images as grayscale float32 frames.\"\"\"\n    loaders = [\n        skdata.astronaut, skdata.camera, skdata.coins, skdata.horse,\n        skdata.hubble_deep_field, skdata.moon, skdata.page, skdata.text,\n        skdata.chelsea, skdata.coffee, skdata.immunohistochemistry,\n        skdata.rocket\n    ]\n    frames = []\n    for loader in loaders:\n        img = loader()\n        if img.ndim == 3:\n            img = color.rgb2gray(img)\n        img = img.astype(np.float32)   # convert bool/uint8 to float32 first\n        img = resize(img, (256, 256), anti_aliasing=True)\n        frames.append(img.astype(np.float32))\n    return frames\n\nclean_frames = load_clean_frames()\nprint(f'Loaded {len(clean_frames)} clean frames — shape: {clean_frames[0].shape}')\n\n# Preview\nfig, axes = plt.subplots(2, 6, figsize=(18, 6))\nfor ax, frame in zip(axes.flat, clean_frames):\n    ax.imshow(frame, cmap='gray')\n    ax.axis('off')\nfig.suptitle('Clean Reference Frames', fontweight='bold')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 · Data Pipeline\n",
    "\n",
    "We use built-in `skimage` images as clean reference frames. In production you would\n",
    "swap in a real video dataset (e.g. DAVIS, Vimeo-90K).\n",
    "\n",
    "Pipeline:\n",
    "1. Load clean frames → normalize to [0, 1]\n",
    "2. Add synthetic Gaussian noise → noisy frames\n",
    "3. Wrap in a `torch.utils.data.Dataset`\n",
    "4. Split train / val / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "anti_aliasing must be False for boolean images",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         frames\u001b[38;5;241m.\u001b[39mappend(img\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frames\n\u001b[0;32m---> 26\u001b[0m clean_frames \u001b[38;5;241m=\u001b[39m load_clean_frames()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(clean_frames)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m clean frames — shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_frames[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Preview\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mload_clean_frames\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     21\u001b[0m         img \u001b[38;5;241m=\u001b[39m color\u001b[38;5;241m.\u001b[39mrgb2gray(img)\n\u001b[0;32m---> 22\u001b[0m     img \u001b[38;5;241m=\u001b[39m resize(img, (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), anti_aliasing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(img\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frames\n",
      "File \u001b[0;32m~/anaconda3/envs/codingenv/lib/python3.11/site-packages/skimage/transform/_warps.py:158\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    152\u001b[0m     anti_aliasing \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m input_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39missubdtype(input_type, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28many\u001b[39m(x \u001b[38;5;241m<\u001b[39m y \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(output_shape, input_shape)))\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m anti_aliasing:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manti_aliasing must be False for boolean images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m factors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(input_shape, output_shape)\n\u001b[1;32m    161\u001b[0m order \u001b[38;5;241m=\u001b[39m _validate_interpolation_order(input_type, order)\n",
      "\u001b[0;31mValueError\u001b[0m: anti_aliasing must be False for boolean images"
     ]
    }
   ],
   "source": [
    "# ── Parameters ────────────────────────────────────────────────────────────────\n",
    "NOISE_STD   = 25 / 255.0   # ← tune: noise level (25/255 is visually noticeable)\n",
    "PATCH_SIZE  = 64            # ← tune: training patch size\n",
    "BATCH_SIZE  = 32\n",
    "NUM_EPOCHS  = 20            # ← tune: increase for better results\n",
    "LR          = 1e-3\n",
    "\n",
    "# ── Load clean images ─────────────────────────────────────────────────────────\n",
    "def load_clean_frames():\n",
    "    \"\"\"Load skimage sample images as grayscale float32 frames.\"\"\"\n",
    "    loaders = [\n",
    "        skdata.astronaut, skdata.camera, skdata.coins, skdata.horse,\n",
    "        skdata.hubble_deep_field, skdata.moon, skdata.page, skdata.text,\n",
    "        skdata.chelsea, skdata.coffee, skdata.immunohistochemistry,\n",
    "        skdata.rocket\n",
    "    ]\n",
    "    frames = []\n",
    "    for loader in loaders:\n",
    "        img = loader()\n",
    "        if img.ndim == 3:\n",
    "            img = color.rgb2gray(img)\n",
    "        img = resize(img, (256, 256), anti_aliasing=True)\n",
    "        frames.append(img.astype(np.float32))\n",
    "    return frames\n",
    "\n",
    "clean_frames = load_clean_frames()\n",
    "print(f'Loaded {len(clean_frames)} clean frames — shape: {clean_frames[0].shape}')\n",
    "\n",
    "# Preview\n",
    "fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "for ax, frame in zip(axes.flat, clean_frames):\n",
    "    ax.imshow(frame, cmap='gray')\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Clean Reference Frames', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDenoiseDataset(Dataset):\n",
    "    \"\"\"Extracts random patches from clean frames and adds Gaussian noise.\"\"\"\n",
    "\n",
    "    def __init__(self, frames, patch_size=64, patches_per_frame=50, noise_std=25/255.0):\n",
    "        self.patches     = []\n",
    "        self.patch_size  = patch_size\n",
    "        self.noise_std   = noise_std\n",
    "\n",
    "        for frame in frames:\n",
    "            H, W = frame.shape\n",
    "            for _ in range(patches_per_frame):\n",
    "                r = np.random.randint(0, H - patch_size)\n",
    "                c = np.random.randint(0, W - patch_size)\n",
    "                patch = frame[r:r+patch_size, c:c+patch_size]\n",
    "                self.patches.append(patch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clean = self.patches[idx]\n",
    "        noise = np.random.normal(0, self.noise_std, clean.shape).astype(np.float32)\n",
    "        noisy = np.clip(clean + noise, 0, 1)\n",
    "        # Add channel dim: (1, H, W)\n",
    "        clean_t = torch.from_numpy(clean).unsqueeze(0)\n",
    "        noisy_t = torch.from_numpy(noisy).unsqueeze(0)\n",
    "        noise_t = torch.from_numpy(noise).unsqueeze(0)  # DnCNN predicts noise\n",
    "        return noisy_t, clean_t, noise_t\n",
    "\n",
    "# ── Train / val / test split ──────────────────────────────────────────────────\n",
    "train_frames = clean_frames[:8]\n",
    "val_frames   = clean_frames[8:10]\n",
    "test_frames  = clean_frames[10:]\n",
    "\n",
    "train_ds = VideoDenoiseDataset(train_frames, PATCH_SIZE, patches_per_frame=100, noise_std=NOISE_STD)\n",
    "val_ds   = VideoDenoiseDataset(val_frames,   PATCH_SIZE, patches_per_frame=50,  noise_std=NOISE_STD)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f'Train patches: {len(train_ds)} | Val patches: {len(val_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 · Classical DSP Baselines\n",
    "\n",
    "Before training the CNN, we establish classical denoising baselines to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img, std=NOISE_STD):\n",
    "    noise = np.random.normal(0, std, img.shape).astype(np.float32)\n",
    "    return np.clip(img + noise, 0, 1)\n",
    "\n",
    "def classical_denoise(noisy):\n",
    "    return {\n",
    "        'Gaussian':       filters.gaussian(noisy, sigma=1.5),\n",
    "        'Wiener':         signal.wiener(noisy, mysize=5),\n",
    "        'Total Variation': restoration.denoise_tv_chambolle(noisy, weight=0.1),\n",
    "    }\n",
    "\n",
    "def compute_metrics(clean, candidate):\n",
    "    p = psnr(clean, candidate, data_range=1.0)\n",
    "    s = ssim(clean, candidate, data_range=1.0)\n",
    "    return p, s\n",
    "\n",
    "# Evaluate on test frames\n",
    "test_img   = test_frames[0]\n",
    "noisy_img  = add_noise(test_img)\n",
    "results    = classical_denoise(noisy_img)\n",
    "\n",
    "print(f'  Noisy input       PSNR={compute_metrics(test_img, noisy_img)[0]:.2f} dB  SSIM={compute_metrics(test_img, noisy_img)[1]:.4f}')\n",
    "for name, denoised in results.items():\n",
    "    p, s = compute_metrics(test_img, denoised)\n",
    "    print(f'  {name:<20} PSNR={p:.2f} dB  SSIM={s:.4f}')\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for ax, (img, title) in zip(axes, [\n",
    "    (test_img,  'Clean'),\n",
    "    (noisy_img, f'Noisy (σ={NOISE_STD:.3f})'),\n",
    "    *[(v, k) for k, v in results.items()]\n",
    "]):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Classical DSP Baselines', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 · DnCNN — Deep Learning Denoiser\n",
    "\n",
    "**DnCNN** (Zhang et al., 2017) uses residual learning: the network predicts\n",
    "the **noise** rather than the clean image.\n",
    "\n",
    "```\n",
    "clean = noisy − DnCNN(noisy)\n",
    "```\n",
    "\n",
    "Architecture:\n",
    "- **Layer 1**: Conv(64) + ReLU\n",
    "- **Layers 2–(D-1)**: Conv(64) + BatchNorm + ReLU\n",
    "- **Layer D**: Conv(1) — outputs predicted noise map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnCNN(nn.Module):\n",
    "    \"\"\"DnCNN: Beyond Gaussian Denoiser (Zhang et al., 2017).\"\"\"\n",
    "\n",
    "    def __init__(self, depth=17, n_filters=64, kernel_size=3):\n",
    "        super().__init__()\n",
    "        pad = kernel_size // 2\n",
    "        layers = []\n",
    "\n",
    "        # First layer: Conv + ReLU (no BN)\n",
    "        layers += [nn.Conv2d(1, n_filters, kernel_size, padding=pad, bias=False),\n",
    "                   nn.ReLU(inplace=True)]\n",
    "\n",
    "        # Middle layers: Conv + BN + ReLU\n",
    "        for _ in range(depth - 2):\n",
    "            layers += [nn.Conv2d(n_filters, n_filters, kernel_size, padding=pad, bias=False),\n",
    "                       nn.BatchNorm2d(n_filters),\n",
    "                       nn.ReLU(inplace=True)]\n",
    "\n",
    "        # Last layer: Conv only — outputs noise map\n",
    "        layers += [nn.Conv2d(n_filters, 1, kernel_size, padding=pad, bias=False)]\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise_pred = self.net(x)\n",
    "        return x - noise_pred          # residual: predicted clean image\n",
    "\n",
    "model = DnCNN(depth=17).to(DEVICE)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'DnCNN parameters: {total_params:,}')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 · Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ── Train ──────────────────────────────────────────────────────────────────\n",
    "    model.train()\n",
    "    t_loss = 0.0\n",
    "    for noisy, clean, _ in train_loader:\n",
    "        noisy, clean = noisy.to(DEVICE), clean.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(noisy)\n",
    "        loss   = criterion(output, clean)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "\n",
    "    # ── Validate ───────────────────────────────────────────────────────────────\n",
    "    model.eval()\n",
    "    v_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for noisy, clean, _ in val_loader:\n",
    "            noisy, clean = noisy.to(DEVICE), clean.to(DEVICE)\n",
    "            output = model(noisy)\n",
    "            v_loss += criterion(output, clean).item()\n",
    "\n",
    "    t_loss /= len(train_loader)\n",
    "    v_loss /= len(val_loader)\n",
    "    train_losses.append(t_loss)\n",
    "    val_losses.append(v_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1:>3}/{NUM_EPOCHS}]  Train Loss: {t_loss:.6f}  Val Loss: {v_loss:.6f}  LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "\n",
    "# ── Plot loss curves ──────────────────────────────────────────────────────────\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses,   label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('DnCNN Training Curves')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 · Evaluation — Classical vs. DnCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dncnn_denoise(model, noisy_img):\n",
    "    \"\"\"Run DnCNN inference on a single grayscale image.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t = torch.from_numpy(noisy_img).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        out = model(t)\n",
    "        out = out.squeeze().cpu().numpy()\n",
    "    return np.clip(out, 0, 1)\n",
    "\n",
    "print('\\n=== Denoising Benchmark (test frames) ===')\n",
    "print(f'{\"Method\":<22} {\"PSNR (dB)\":>10} {\"SSIM\":>8}')\n",
    "print('-' * 42)\n",
    "\n",
    "all_results = {}\n",
    "for test_img in test_frames:\n",
    "    noisy = add_noise(test_img)\n",
    "    classical = classical_denoise(noisy)\n",
    "    cnn_out   = dncnn_denoise(model, noisy.astype(np.float32))\n",
    "\n",
    "    methods = {'Noisy': noisy, **classical, 'DnCNN': cnn_out}\n",
    "    for name, img in methods.items():\n",
    "        p, s = compute_metrics(test_img, img)\n",
    "        all_results.setdefault(name, []).append((p, s))\n",
    "\n",
    "for name, scores in all_results.items():\n",
    "    avg_p = np.mean([s[0] for s in scores])\n",
    "    avg_s = np.mean([s[1] for s in scores])\n",
    "    print(f'{name:<22} {avg_p:>10.2f} {avg_s:>8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visual comparison on one test frame ───────────────────────────────────────\n",
    "test_img  = test_frames[0]\n",
    "noisy_img = add_noise(test_img)\n",
    "classical = classical_denoise(noisy_img)\n",
    "cnn_out   = dncnn_denoise(model, noisy_img.astype(np.float32))\n",
    "\n",
    "imgs   = [test_img, noisy_img, *classical.values(), cnn_out]\n",
    "titles = ['Clean', f'Noisy (σ={NOISE_STD:.3f})', *classical.keys(), 'DnCNN']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(imgs), figsize=(4 * len(imgs), 4))\n",
    "for ax, img, title in zip(axes, imgs, titles):\n",
    "    p, s = compute_metrics(test_img, img)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'{title}\\nPSNR={p:.1f} dB', fontsize=9)\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Classical DSP vs. DnCNN', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 · Video Processing Pipeline\n",
    "\n",
    "Apply the trained DnCNN frame-by-frame to a video.\n",
    "- **Option A**: Use the synthetic video generated below\n",
    "- **Option B**: Swap in your own `.mp4` path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Option A: Generate a synthetic noisy video from test frames ───────────────\n",
    "SYNTHETIC_VIDEO = 'synthetic_noisy.avi'\n",
    "H, W = 256, 256\n",
    "fps  = 10\n",
    "\n",
    "writer = cv2.VideoWriter(SYNTHETIC_VIDEO, cv2.VideoWriter_fourcc(*'XVID'), fps, (W, H), isColor=False)\n",
    "source_frames = []\n",
    "for frame in test_frames * 5:   # repeat to make a longer clip\n",
    "    noisy = add_noise(frame)\n",
    "    uint8 = (noisy * 255).astype(np.uint8)\n",
    "    writer.write(uint8)\n",
    "    source_frames.append((frame, noisy))\n",
    "writer.release()\n",
    "print(f'Synthetic video saved: {SYNTHETIC_VIDEO}  ({len(source_frames)} frames)')\n",
    "\n",
    "# ── Option B: use your own video ──────────────────────────────────────────────\n",
    "# SYNTHETIC_VIDEO = 'your_video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_path, output_path, model, noise_std=None):\n",
    "    \"\"\"\n",
    "    Read a video frame-by-frame, denoise with DnCNN, write output video.\n",
    "    If noise_std is set, synthetic noise is added before denoising.\n",
    "    \"\"\"\n",
    "    cap  = cv2.VideoCapture(input_path)\n",
    "    fps  = cap.get(cv2.CAP_PROP_FPS)\n",
    "    W    = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H    = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (W, H), isColor=False)\n",
    "\n",
    "    frame_psnrs = []\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert to grayscale float\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
    "\n",
    "        if noise_std:\n",
    "            gray = np.clip(gray + np.random.normal(0, noise_std, gray.shape).astype(np.float32), 0, 1)\n",
    "\n",
    "        # DnCNN inference\n",
    "        denoised = dncnn_denoise(model, gray)\n",
    "        denoised_u8 = (denoised * 255).astype(np.uint8)\n",
    "        out.write(denoised_u8)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f'Denoised video saved: {output_path}  ({total} frames)')\n",
    "\n",
    "process_video(SYNTHETIC_VIDEO, 'denoised_output.avi', model)\n",
    "\n",
    "# ── Visualize sample frames ───────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(SYNTHETIC_VIDEO)\n",
    "sample_noisy, sample_denoised = [], []\n",
    "for i in range(5):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
    "    sample_noisy.append(gray)\n",
    "    sample_denoised.append(dncnn_denoise(model, gray))\n",
    "cap.release()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 6))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(sample_noisy[i],    cmap='gray'); axes[0, i].set_title(f'Noisy frame {i+1}');    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(sample_denoised[i], cmap='gray'); axes[1, i].set_title(f'Denoised frame {i+1}'); axes[1, i].axis('off')\n",
    "fig.suptitle('Video Frame Denoising — DnCNN', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 · Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save ──────────────────────────────────────────────────────────────────────\n",
    "torch.save(model.state_dict(), 'dncnn_weights.pth')\n",
    "print('Model saved to dncnn_weights.pth')\n",
    "\n",
    "# ── Load (example) ────────────────────────────────────────────────────────────\n",
    "# model_loaded = DnCNN(depth=17).to(DEVICE)\n",
    "# model_loaded.load_state_dict(torch.load('dncnn_weights.pth', map_location=DEVICE))\n",
    "# model_loaded.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}